import torch
from torch import tensor
import torch.nn as nn
import torch.nn.init as init
import numpy as np
import random
import os

# --- 1. Seeding Function and Call ---
def set_seed(seed_value=42):
    """Sets the seed for reproducibility across CPU and GPU operations."""
    random.seed(seed_value)
    np.random.seed(seed_value)
    torch.manual_seed(seed_value)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(seed_value)
        torch.cuda.manual_seed_all(seed_value)
        torch.backends.cudnn.deterministic = True
        torch.backends.cudnn.benchmark = False

set_seed(seed_value=2)
# ------------------------------------

# --- Setup from your code (Forward Pass) ---

# Data: Transposed for (Features x Batch)
inputs = ((-0.8 ,1.7 ,0.2), (0.3, -1.5, 1.2), (1.9,0, -0.7), (-1.2, -0.4, 2), (0.6,1.1, -0.3))
X = torch.tensor(inputs, dtype=torch.float32).t() # (3, 5)

outputs = ((0,1),(1,0),(0,1),(1,0),(0,1))
Y = torch.tensor(outputs, dtype=torch.float32).t() # (2, 5)

# Initialize Weights with gradient tracking (Will be the same due to seed)
w2 = torch.empty(4, 3); init.xavier_uniform_(w2); w2.requires_grad = True # W(2): 4x3
print(f'W2 is {w2}')
w3 = torch.empty(3, 4); init.kaiming_uniform_(w3, nonlinearity='relu'); w3.requires_grad = True # W(3): 3x4
print(f'W3 is {w3}')
w4 = torch.empty(2, 3); init.xavier_uniform_(w4); w4.requires_grad = True # W(4): 2x3
print(f'W4 is {w4}')
# Biases: Repeated to match batch size M=5
b2 = torch.zeros(4, 1, requires_grad=True).repeat(1, 5) # b(2): 4x5
print(f'b2 is {b2}')
b3 = torch.zeros(3, 1, requires_grad=True).repeat(1, 5) # b(3): 3x5
print(f'b3 is {b3}')
b4 = torch.zeros(2, 1, requires_grad=True).repeat(1, 5) # b(4): 2x5
print(f'b4 is {b4}')

# Forward Pass (Re-executed to ensure fresh computational graph)
Z2 = w2 @ X + b2
print(f'Z2 is {Z2}')
A2 = torch.tanh(Z2)
print(f"A2 is {A2}")

Z3 = w3 @ A2 + b3
print(f'Z3 is {Z3}')
A3 = torch.relu(Z3)
print(f"A3 is {A3}")

Z4 = w4 @ A3 + b4
print(f'Z4 is {Z4}')
A4 = torch.sigmoid(Z4)
print(f"A4 is {A4}")

# Loss Calculation
loss = nn.functional.binary_cross_entropy(A4.t(), Y.t(), reduction='sum') / X.shape[1]
print(f"Initial Loss: {loss.item():.4f}")


# --- Adam Optimizer Parameters and State ---
eta = 0.01 # Learning Rate
beta1 = 0.9
beta2 = 0.999
epsilon = 1e-8
t = 1 # Iteration count

# Adam moment initialization
adam_state = {
    w4: {'m': torch.zeros_like(w4), 'v': torch.zeros_like(w4)},
    b4: {'m': torch.zeros_like(b4), 'v': torch.zeros_like(b4)},
    w3: {'m': torch.zeros_like(w3), 'v': torch.zeros_like(w3)},
    b3: {'m': torch.zeros_like(b3), 'v': torch.zeros_like(b3)},
    w2: {'m': torch.zeros_like(w2), 'v': torch.zeros_like(w2)},
    b2: {'m': torch.zeros_like(b2), 'v': torch.zeros_like(b2)},
}

# --- Manual Backpropagation (Backward Pass) ---

# Layer 4 (Output: Sigmoid + BCE Loss)
M = X.shape[1]
delta4 = (A4 - Y) / M
print(f' delta4 is {delta4}')
GW4 = delta4 @ A3.t()
print(f'GW4 is {GW4}')
Gb4 = delta4.mean(dim=1, keepdim=True).repeat(1, M)
print(f'Gb4 is {Gb4}')

# Layer 3 (Hidden 2: ReLU)
local_grad3 = (Z3 > 0).float()
print(f'local_grad3 is {local_grad3}')
delta3_backprop = w4.t() @ delta4
print(f'delta3_backprop is {delta3_backprop}')
delta3 = delta3_backprop * local_grad3
print(f'delta3 is {delta3}')

GW3 = delta3 @ A2.t()
print(f'GW3 is {GW3}')
Gb3 = delta3.mean(dim=1, keepdim=True).repeat(1, M)
print(f'Gb3 is {Gb3}')


# Layer 2 (Hidden 1: Tanh)
local_grad2 = 1 - A2.pow(2)
print(f'local_grad2 is {local_grad2}')
delta2_backprop = w3.t() @ delta3
print(f'delta2_backprop is {delta2_backprop}')
delta2 = delta2_backprop * local_grad2
print(f'delta2 is {delta2}')

GW2 = delta2 @ X.t()
print(f'GW2 is {GW2}')
Gb2 = delta2.mean(dim=1, keepdim=True).repeat(1, M)
print(f'Gb2 is {Gb2}')

# --- Apply Adam Optimizer Update ---

parameters = [w4, b4, w3, b3, w2, b2]
gradients = [GW4, Gb4, GW3, Gb3, GW2, Gb2]
parameter_names = ['w4', 'b4', 'w3', 'b3', 'w2', 'b2']

def adam_update(P, G):
    state = adam_state[P]
    m_old = state['m']
    v_old = state['v']

    # 1. Update moments
    m_new = beta1 * m_old + (1 - beta1) * G
    v_new = beta2 * v_old + (1 - beta2) * G.pow(2)

    # 2. Bias Correction
    m_hat = m_new / (1 - beta1**t)
    v_hat = v_new / (1 - beta2**t)

    # 3. Update parameter
    P_update = -eta * m_hat / (torch.sqrt(v_hat) + epsilon)
    P.data.add_(P_update)

    # 4. Save state
    state['m'] = m_new
    state['v'] = v_new
    # Print the updated parameter
    print(f"Updated parameter:\n{P}")


# Apply update to all parameters
for i, (P, G) in enumerate(zip(parameters, gradients)):
    print(f"Updating parameter: {parameter_names[i]}")
    adam_update(P, G)

# --- Re-run Forward Pass to check new Loss ---
Z2_new = w2 @ X + b2
A2_new = torch.tanh(Z2_new)
Z3_new = w3 @ A2_new + b3
A3_new = torch.relu(Z3_new)
Z4_new = w4 @ A3_new + b4
A4_new = torch.sigmoid(Z4_new)

print(f"A4_new is {A4_new}")

loss_new = nn.functional.binary_cross_entropy(A4_new.t(), Y.t(), reduction='sum') / X.shape[1]
print(f"New Loss after Adam update: {loss_new.item():.4f}")

